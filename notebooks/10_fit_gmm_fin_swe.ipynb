{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI56pQeVIwox"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from shapely.geometry import shape\n",
    "import nimfa\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style (optional, but professional)\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT5qHl-2KaeM"
   },
   "source": [
    "# Define the folder path where raw sensor data files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCF3-eWDI_Qj"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path relative to the notebook or project root\n",
    "FolderPath = Path(\"../Data/FIN-SWE/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8z9UXtNKgnc"
   },
   "source": [
    "# List of sensor IDs corresponding to traffic sensors for Finland and Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAfI04ikKhPy"
   },
   "outputs": [],
   "source": [
    "Sensors = [\"1433\", \"1432\", \"1435\", \"1436\", \"1431\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVIFpnl-KnFR"
   },
   "source": [
    "# Loop through each sensor, read its CSV file, and concatenate data into 'all_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KoV59maKnbg"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store combined data from all sensors\n",
    "all_data = pd.DataFrame()\n",
    "for s in Sensors:\n",
    "    # Read CSV file for the current sensor\n",
    "    sensor_data = pd.read_csv(\n",
    "        FolderPath + s + '_by_length_minute.csv',\n",
    "        sep=',',\n",
    "        usecols=['TMS point id', 'year', 'days', 'hour', 'minute', 'v_type', 'direction', 'total_vehicles', 'date'],\n",
    "        parse_dates=['date']\n",
    "    )\n",
    "    # Concatenate current sensor data to all_data DataFrame\n",
    "    all_data = pd.concat([all_data, sensor_data])\n",
    "\n",
    "# Select vehicle type for analysis: options are 'Small', 'Heavy', 'Total', or 'Both'\n",
    "WhichVehicles = 'Small'\n",
    "\n",
    "# Create a copy of the combined data for processing\n",
    "data = all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI4saZTtMAXH"
   },
   "source": [
    "# Define the date range for filtering the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtZKY2V5L82k"
   },
   "outputs": [],
   "source": [
    "min_date = datetime.datetime(2017, 1, 1)\n",
    "max_date = datetime.datetime(2023, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18nJYNkoMCkb"
   },
   "source": [
    "# Define a function to aggregate and preprocess the sensor data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwP8XILWMD0g"
   },
   "outputs": [],
   "source": [
    "def agg_APIdata_SWEFIN(data, WhichVehicles, min_date=datetime.datetime(2017, 1, 1), max_date=datetime.datetime(2023, 5, 22)):\n",
    "    \"\"\"\n",
    "    Aggregates raw sensor data by vehicle type and time, and formats it for further analysis.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Raw traffic sensor data.\n",
    "        WhichVehicles (str): Filter for vehicle types. Options: 'Small', 'Heavy', 'Total', 'Both'.\n",
    "        min_date (datetime): Minimum datetime filter.\n",
    "        max_date (datetime): Maximum datetime filter.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated and time-indexed vehicle counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select relevant columns and reset index\n",
    "    agg_data = data[['TMS point id', 'hour', 'minute', 'v_type', 'direction', 'total_vehicles', 'date']].reset_index(drop=True).copy()\n",
    "\n",
    "    # Filter or aggregate data based on vehicle type choice\n",
    "    if WhichVehicles == 'Total':\n",
    "        # Sum total vehicles regardless of vehicle type\n",
    "        agg_data = agg_data.groupby(['TMS point id', 'hour', 'minute', 'direction', 'date'])['total_vehicles'].sum().reset_index()\n",
    "    elif WhichVehicles == 'Small':\n",
    "        # Select only small vehicles (length < 5.6m)\n",
    "        agg_data = agg_data[agg_data['v_type'] == '<5.6m'].drop(columns='v_type').reset_index(drop=True)\n",
    "    elif WhichVehicles == 'Heavy':\n",
    "        # Select only heavy vehicles (length >= 5.6m)\n",
    "        agg_data = agg_data[agg_data['v_type'] == '>=5.6m'].drop(columns='v_type').reset_index(drop=True)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    agg_data = agg_data.rename({'TMS point id': 'sensor_id', 'direction': 'sensor_dir'}, axis=1)\n",
    "\n",
    "    # Define origin and destination country based on direction\n",
    "    agg_data['dest_country'] = np.where(agg_data['sensor_dir'] == 2, 'FIN', 'SWE')\n",
    "    agg_data['origin_country'] = np.where(agg_data['sensor_dir'] == 2, 'SWE', 'FIN')\n",
    "\n",
    "    # Combine sensor ID and country info for origin and destination\n",
    "    agg_data[\"sensor_id\"] = agg_data[\"sensor_id\"].astype(str)\n",
    "    agg_data['sensor_origin'] = agg_data[['sensor_id', 'origin_country']].agg(', '.join, axis=1)\n",
    "    agg_data['sensor_destination'] = agg_data[['sensor_id', 'dest_country']].agg(', '.join, axis=1)\n",
    "\n",
    "    # Convert 'hour' and 'minute' columns to timedeltas and add to 'date' to get precise timestamp\n",
    "    agg_data[\"minute\"] = pd.to_timedelta(agg_data[\"minute\"], unit=\"min\")\n",
    "    agg_data[\"hour\"] = pd.to_timedelta(agg_data[\"hour\"], unit=\"h\")\n",
    "    agg_data['date'] = agg_data['date'] + agg_data['hour'] + agg_data['minute']\n",
    "\n",
    "    # Filter data within the specified date range\n",
    "    agg_data = agg_data[(agg_data.date > min_date) & (agg_data.date < max_date)].copy().reset_index(drop=True)\n",
    "\n",
    "    # Update min and max date from filtered data\n",
    "    min_date = agg_data.date.min()\n",
    "    max_date = agg_data.date.max()\n",
    "\n",
    "    # If 'Both' vehicle types requested, return data as is with vehicle type info\n",
    "    if WhichVehicles == 'Both':\n",
    "        agg_data = agg_data[['sensor_origin', 'sensor_destination', 'date', 'v_type', 'total_vehicles']].copy()\n",
    "    else:\n",
    "        # Otherwise, aggregate total vehicles per sensor origin-destination pair and timestamp\n",
    "        agg_data = agg_data.groupby(['sensor_origin', 'sensor_destination', 'date'])['total_vehicles'].sum().reset_index()\n",
    "\n",
    "        # Remove rows with missing vehicle counts (NaNs)\n",
    "        rm_idx = np.where(np.isnan(agg_data.total_vehicles))\n",
    "        agg_data = agg_data.drop(index=rm_idx[0])\n",
    "\n",
    "        # Convert total vehicles to integers\n",
    "        agg_data['total_vehicles'] = agg_data['total_vehicles'].apply(int)'\"'\n",
    "\n",
    "        # Define a function to reindex data to a complete datetime range with 1-minute frequency, filling missing with zeros\n",
    "        def fill_missing_timepoints(x):\n",
    "            return x.reindex(pd.date_range(min_date, max_date, name='date', freq='1min'), fill_value=0)\n",
    "\n",
    "        # Apply the reindexing function grouped by origin-destination pairs\n",
    "        agg_data = (\n",
    "            agg_data.set_index('date')\n",
    "            .groupby([\"sensor_origin\", \"sensor_destination\"])[\"total_vehicles\"]\n",
    "            .apply(fill_missing_timepoints)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Pivot the table to have datetime as columns and origin-destination pairs as rows\n",
    "        agg_data = agg_data.pivot_table(\n",
    "            index=[\"sensor_origin\", \"sensor_destination\"],\n",
    "            columns=[\"date\"],\n",
    "            values=[\"total_vehicles\"]\n",
    "        ).droplevel(level=0, axis=1)  # Drop top level of column MultiIndex\n",
    "\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "begU3p_qMjo3"
   },
   "source": [
    "# Run the aggregation function on all_data for 'Small' vehicles and given date range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvecPE07Mj-j"
   },
   "outputs": [],
   "source": [
    "agg_data = agg_APIdata_SWEFIN(\n",
    "    all_data,\n",
    "    WhichVehicles='Small',\n",
    "    min_date=datetime.datetime(2017, 1, 1),\n",
    "    max_date=datetime.datetime(2023, 12, 31)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De3q_7NJMl6y"
   },
   "source": [
    "# Fit Gaussian Mixture Models to the aggregated data for the pre-COVID period (2017 to March 1, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj0SMN4lMonJ"
   },
   "outputs": [],
   "source": [
    "models_pre, data_pre = utils.fit_period(\n",
    "    agg_data,\n",
    "    d1=datetime.date(2017, 1, 1),\n",
    "    d2=datetime.date(2020, 3, 1),\n",
    "    hourly=False,\n",
    "    nSamp=10000,\n",
    "    Normalize=False,\n",
    "    N=10,\n",
    "    seed=1923,\n",
    "    FitMethod='Bayesian'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkBzBDHHMsl-"
   },
   "source": [
    "# Save the fitted models and processed data to disk for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUPnHxMxMtGx"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".../Data/FinSwe_GMM\")  # cleaner & portable\n",
    "MODELS_FILE = BASE_DIR / \"models_fin.pkl\"\n",
    "DATA_FILE = BASE_DIR / \"data_fin.pkl\"\n",
    "AGG_DATA_FILE = BASE_DIR / \"agg_data_fin.pkl\"\n",
    "\n",
    "# Save\n",
    "models_pre.to_pickle(MODELS_FILE)\n",
    "data_pre.to_pickle(DATA_FILE)\n",
    "agg_data.to_pickle(AGG_DATA_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDJ1Rxd7OUkrqhLOlZsnVs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
